{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_L_qHeljt329"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mlBxwaWAb3xv"
      },
      "outputs": [],
      "source": [
        "!pip install --find-links https://download.pytorch.org/whl/torch_stable.html wandb einops>=0.4 vector-quantize-pytorch>=0.10.15 librosa==0.10.0 torchlibrosa==0.1.0 ftfy tqdm transformers encodec==0.1.1 gdown accelerate>=0.24.0 beartype joblib h5py scikit-learn wget"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir(\"/content/drive/open_musiclm\")"
      ],
      "metadata": {
        "id": "VKCEBh_r1pJR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GyaazPydyPMk"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "import torchaudio\n",
        "from einops import rearrange\n",
        "from torchaudio.functional import resample\n",
        "from open_musiclm.config import (create_clap_quantized_from_config,\n",
        "                                 create_coarse_transformer_from_config,\n",
        "                                 create_semcoarsetosem_transformer_from_config,\n",
        "                                 create_encodec_from_config,\n",
        "                                 create_hubert_kmeans_from_config,\n",
        "                                 my_load_model_config,load_model_config)\n",
        "from open_musiclm.open_musiclm import (SemcoarsetosemStage,CoarseStage,\n",
        "                                       get_or_compute_clap_token_ids,\n",
        "                                       get_or_compute_acoustic_token_ids,\n",
        "                                       get_or_compute_semantic_token_ids)\n",
        "from open_musiclm.utils import int16_to_float32, float32_to_int16, zero_mean_unit_var_norm\n",
        "from scripts.train_utils import disable_print\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P0WH3AWb2rX5"
      },
      "outputs": [],
      "source": [
        "def make_file_name(file_string):\n",
        "  file_string=str(file_string)\n",
        "  # 문자열을 \"/\"을 기준으로 분할하여 리스트로 만듭니다.\n",
        "  parts = file_string.split(\"/\")\n",
        "\n",
        "  # 파일 이름 부분에서 원하는 정보 추출\n",
        "  file_name = parts[-1]\n",
        "  file_name_parts = file_name.split(\"_\")\n",
        "  number = file_name_parts[-1].split(\".\")[0]\n",
        "  description_with_underscore = parts[-3].replace(\" \", \"_\")\n",
        "  # 새로운 파일 이름 생성\n",
        "  new_file_name = f\"{description_with_underscore}_{number}\"\n",
        "  return new_file_name\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i54D6UrsybhZ"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "folder = \"/content/drive/MyDrive/data/instrument/train/instrument_train\"\n",
        "path = Path(folder)\n",
        "train_files = [file for file in path.glob(f'**/vocals/*.wav')]\n",
        "print(len(train_files))\n",
        "\n",
        "\n",
        "all_files = train_files\n",
        "print(\"Total files found:\", len(all_files))\n",
        "\n",
        "random.shuffle(all_files)\n",
        "print(all_files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "CYk5JkTOyU2c"
      },
      "outputs": [],
      "source": [
        "my_model_path=\"real_semcoarsetosem.transformer.5170.pt\"\n",
        "my_model_config=\"my_musiclm_for_semcoarsetosem.json\"\n",
        "my_model_config = my_load_model_config(my_model_config)\n",
        "\n",
        "# coarse_path = \"/content/drive/MyDrive/my_code/mymusiclm/my-open-musiclm-main/explorer/wandb/run-20240223_102209-e01qb72w/files/coarse_generation_test.transformer.900.pt\"\n",
        "rvq_path=\"/content/drive/MyDrive/data/weight/clap.rvq.950_no_fusion.pt\"\n",
        "kmeans_path=\"/content/drive/MyDrive/data/weight/kmeans_10s_no_fusion.joblib\"\n",
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "clap = create_clap_quantized_from_config(my_model_config, rvq_path, device)\n",
        "wav2vec = create_hubert_kmeans_from_config(my_model_config, kmeans_path, device)\n",
        "encodec_wrapper = create_encodec_from_config(my_model_config, device)\n",
        "semcoarsetosem_transformer = create_semcoarsetosem_transformer_from_config(my_model_config,my_model_path, device)\n",
        "\n",
        "semcoarsetosem_stage = SemcoarsetosemStage(\n",
        "    semcoarsetosem_transformer=semcoarsetosem_transformer,\n",
        "    neural_codec=encodec_wrapper,\n",
        "    wav2vec=wav2vec,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xZhHQF-Z1TH6"
      },
      "outputs": [],
      "source": [
        "model_config=\"/content/drive/MyDrive/my_code/mymusiclm/open-musiclm-main/configs/model/musiclm_large_small_context.json\"\n",
        "model_config = load_model_config(model_config)\n",
        "duration=5\n",
        "results_folder = \"/content/drive/MyDrive/my_code/mymusiclm/my-open-musiclm-main/explorer/music_generation\"\n",
        "Path(results_folder).mkdir(parents=True, exist_ok=True)\n",
        "coarse_path=\"/content/drive/MyDrive/data/weight/coarse.transformer.18000.pt\"\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "clap = create_clap_quantized_from_config(model_config,rvq_path, device)\n",
        "wav2vec = create_hubert_kmeans_from_config(model_config, kmeans_path, device)\n",
        "encodec_wrapper = create_encodec_from_config(model_config, device)\n",
        "coarse_transformer = create_coarse_transformer_from_config(model_config, coarse_path, device)\n",
        "torch.manual_seed(42)\n",
        "coarse_stage = CoarseStage(\n",
        "        coarse_transformer=coarse_transformer,\n",
        "        neural_codec=encodec_wrapper,\n",
        "        wav2vec=wav2vec,\n",
        "        clap=clap\n",
        ")\n",
        "text=[\"Diverse kinds of instrument and richness\"]\n",
        "clap_token_ids = get_or_compute_clap_token_ids(None, clap, None, text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7_neXYbT44IV"
      },
      "outputs": [],
      "source": [
        "import librosa\n",
        "import numpy as np\n",
        "import soundfile as sf\n",
        "import torchaudio\n",
        "import torch\n",
        "from torchaudio.functional import resample\n",
        "\n",
        "def int16_to_float32(x):\n",
        "    return (x / 32767.0).type(torch.float32)\n",
        "\n",
        "def float32_to_int16(x):\n",
        "    x = torch.clamp(x, min=-1., max=1.)\n",
        "    return (x * 32767.).type(torch.int16)\n",
        "\n",
        "def zero_mean_unit_var_norm(x):\n",
        "    return (x - x.mean(dim=-1, keepdim=True)) / torch.sqrt(x.var(dim=-1, keepdim=True) + 1e-7)\n",
        "\n",
        "def my_linear_mixing(audio1, audio2, output_file):\n",
        "    # 오디오 파일 로드\n",
        "\n",
        "    y1, sr1 = torchaudio.load(audio1)\n",
        "    y2, sr2 = torchaudio.load(audio2)\n",
        "    print(\"sr1\",sr1,\"sr2,\",sr2)\n",
        "    if y2.shape[0] > 1:\n",
        "            y2 = torch.mean(y2, dim=0).unsqueeze(0)\n",
        "    if y1.shape[0] > 1:\n",
        "            y1 = torch.mean(y1, dim=0).unsqueeze(0)\n",
        "     # 오디오 길이 일치화\n",
        "\n",
        "    y2 = resample(y2, sr2, sr1)\n",
        "    y2 = int16_to_float32(float32_to_int16(y2))\n",
        "\n",
        "    min_length = min(y1.shape[1], y2.shape[1])\n",
        "\n",
        "\n",
        "    y1 = y1[:,:min_length]\n",
        "\n",
        "    y2 =y2[:, :min_length]\n",
        "\n",
        "    # 선형으로 엮기\n",
        "    mixed_audio = y1 * 0.5 + y2\n",
        "    mixed_audio=mixed_audio.squeeze()\n",
        "    # 결과 저장\n",
        "    sf.write(output_file, mixed_audio, sr1)\n",
        "    print(f\"{output_file} 에 믹스 파일 저장함\")\n",
        "\n",
        "def linear_mixing(audio1, audio2, output_file):\n",
        "    # 오디오 파일 로드\n",
        "    y1, sr1 = librosa.load(audio1, sr=None)\n",
        "    y2, sr2 = librosa.load(audio2, sr=None)\n",
        "    # y3, sr3 = librosa.load(audio3, sr=None)\n",
        "\n",
        "    # 오디오 길이 일치화\n",
        "    min_length = min(len(y1), len(y2))\n",
        "    y1 = y1[:min_length]\n",
        "    y2 = y2[:min_length]\n",
        "    # y3 = y3[:min_length]\n",
        "\n",
        "    # 선형으로 엮기\n",
        "    mixed_audio = y1 * 0.8 + y2\n",
        "    # 결과 저장\n",
        "    sf.write(output_file, mixed_audio, sr1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qht5BVXm0CzN"
      },
      "outputs": [],
      "source": [
        "cnt=0\n",
        "for audio_path in all_files:\n",
        "    cnt+=1\n",
        "    name=make_file_name(audio_path)\n",
        "    data, sample_hz = torchaudio.load(audio_path)\n",
        "\n",
        "    if data.shape[0] > 1:\n",
        "        data = torch.mean(data, dim=0).unsqueeze(0)\n",
        "\n",
        "    target_length = int(10 * sample_hz)\n",
        "    normalized_data = zero_mean_unit_var_norm(data)\n",
        "\n",
        "    data = data[:, :target_length]\n",
        "    normalized_data = normalized_data[: , :target_length]\n",
        "    audio_for_encodec = resample(data, sample_hz, encodec_wrapper.sample_rate)\n",
        "    audio_for_wav2vec = resample(normalized_data, sample_hz, wav2vec.target_sample_hz)\n",
        "\n",
        "    audio_for_encodec = int16_to_float32(float32_to_int16(audio_for_encodec)).to(device)\n",
        "    audio_for_wav2vec = int16_to_float32(float32_to_int16(audio_for_wav2vec)).to(device)\n",
        "    vocals_semantic_token_ids = get_or_compute_semantic_token_ids(None, audio_for_wav2vec, wav2vec)\n",
        "    vocals_coarse_token_ids, _ = get_or_compute_acoustic_token_ids(None, None, audio_for_encodec, encodec_wrapper, model_config.global_cfg.num_coarse_quantizers)\n",
        "\n",
        "    generated_inst_semantic_ids = semcoarsetosem_stage.generate(\n",
        "        vocals_semantic_token_ids=vocals_semantic_token_ids,\n",
        "        vocals_coarse_token_ids=vocals_coarse_token_ids,\n",
        "        # max_time_steps=10,\n",
        "        max_time_steps=200,\n",
        "        temperature=0.90,\n",
        "    )\n",
        "    print(generated_inst_semantic_ids.shape)\n",
        "    generated_wave = coarse_stage.generate(\n",
        "        clap_token_ids=clap_token_ids,\n",
        "        semantic_token_ids=generated_inst_semantic_ids.squeeze(2),\n",
        "        #gt_semantic_token_ids[0].unsqueeze(0),\n",
        "        coarse_token_ids=None,\n",
        "        # max_time_steps=10,\n",
        "        max_time_steps=duration*75,\n",
        "        # max_time_steps=int(model_config.global_cfg.coarse_audio_length_seconds * 75),\n",
        "        reconstruct_wave=True,\n",
        "        include_eos_in_output=False,\n",
        "        append_eos_to_conditioning_tokens=True,\n",
        "        temperature=0.95,\n",
        "    )\n",
        "\n",
        "    generated_wave = rearrange(generated_wave, 'b n -> b 1 n').detach().cpu()\n",
        "\n",
        "    for i, wave in enumerate(generated_wave):\n",
        "        torchaudio.save(f'{results_folder}/{name}.wav', wave, encodec_wrapper.sample_rate)\n",
        "        print(\"=============================================================\")\n",
        "        print(f\"{results_folder}/{name}.wav 에 최종 만듦\\n\\n\\n\")\n",
        "    my_inst=f\"{results_folder}/{name}.wav\"\n",
        "    gt_inst=audio_path.replace(\"vocals\",\"instrument\")\n",
        "    linear_mixing(gt_inst,audio_path,f\"{results_folder}/{name}_mixed_gt.wav\")\n",
        "    my_linear_mixing(my_inst,audio_path,f\"{results_folder}/{name}_mixed.wav\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}